{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilities\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18816, 15), (6273, 15), (18816, 5), (6273, 5))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\suhan\\Coding_Projects\\Machine_Learning\\Cyberbullying_Classification\\Data\\finalData.csv\")\n",
    "\n",
    "# Normalizing the input matrix.\n",
    "X = df.drop('Labels',axis=1)\n",
    "X = normalize(X)\n",
    "\n",
    "# Splitting into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,df['Labels'])\n",
    "\n",
    "# One-Hot encoding\n",
    "\n",
    "y_train = to_categorical(y_train,num_classes=5)\n",
    "y_test = to_categorical(y_test,num_classes=5)\n",
    "\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit # Sigmoid function\n",
    "from scipy.special import softmax # Softmax function\n",
    "\n",
    "# Multinomial cross entropy loss\n",
    "\n",
    "def crossEntropyLoss(Y, Y_hat):\n",
    "\n",
    "    L = np.sum(np.multiply(Y,np.log(Y_hat)))\n",
    "    m = Y.shape[0]\n",
    "    L /= -m\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining variables\n",
    "\n",
    "n_i = 15\n",
    "n_h = 7\n",
    "classes = 5\n",
    "learning_rate = 0.001\n",
    "\n",
    "# Defining Parameters\n",
    "\n",
    "W1 = np.random.randn(n_h, n_i)\n",
    "b1 = np.zeros((n_h, 1))\n",
    "W2 = np.random.randn(classes, n_h)\n",
    "b2 = np.zeros((classes, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  0.0  Cost :  0.2774575383540472\n",
      "Epoch :  1.0  Cost :  0.13706180118965783\n",
      "Epoch :  2.0  Cost :  0.1353677980409846\n",
      "Epoch :  3.0  Cost :  0.13451459122165718\n",
      "Epoch :  4.0  Cost :  0.1340994415077465\n",
      "Epoch :  5.0  Cost :  0.13388637674325235\n",
      "Epoch :  6.0  Cost :  0.13375798737980382\n",
      "Epoch :  7.0  Cost :  0.13367059971856424\n",
      "Epoch :  8.0  Cost :  0.13360562087256372\n",
      "Epoch :  9.0  Cost :  0.13355400087252528\n",
      "Epoch :  10.0  Cost :  0.13351095159192483\n"
     ]
    }
   ],
   "source": [
    "num_iters = 1000\n",
    "num_datapoints = X_train.shape[0]\n",
    "\n",
    "cost = 0\n",
    "\n",
    "for i in range(num_iters+1):\n",
    "\n",
    "    temp_cost = 0\n",
    "\n",
    "    for j in range(num_datapoints):\n",
    "        \n",
    "        # Front Propogation\n",
    "\n",
    "        Z1 = (W1 @ X_train[j].reshape(15, 1)) + b1\n",
    "        A1 = expit(Z1)\n",
    "        Z2 = (W2 @ A1) + b2\n",
    "        A2 = softmax(Z2)\n",
    "\n",
    "        # Cost calculation\n",
    "        \n",
    "        temp_cost += crossEntropyLoss(y_train[j].reshape(5,1),A2)\n",
    "        \n",
    "        # Back Propogation\n",
    "\n",
    "        dZ2 = A2-(y_train[j].reshape(5,1))\n",
    "        dW2 = (dZ2 @ A1.T)\n",
    "        db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "\n",
    "        dA1 = (W2.T @ dZ2)\n",
    "        dZ1 = dA1 * expit(Z1) * (1 - expit(Z1))\n",
    "        dW1 = (dZ1 @ (X_train[j].reshape(15, 1)).T)\n",
    "        db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "\n",
    "        # Parameter updating\n",
    "\n",
    "        W2 = W2 - learning_rate * dW2\n",
    "        b2 = b2 - learning_rate * db2\n",
    "        W1 = W1 - learning_rate * dW1\n",
    "        b1 = b1 - learning_rate * db1\n",
    "\n",
    "    if(i%100 == 0):\n",
    "        cost += temp_cost/num_datapoints\n",
    "        print(\"Epoch : \",i/100,\" Cost : \",cost)\n",
    "        cost = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.7425474254742548\n"
     ]
    }
   ],
   "source": [
    "n_testDatapoints = X_test.shape[0]\n",
    "\n",
    "Accuracy = 0\n",
    "\n",
    "for i in range(n_testDatapoints):\n",
    "\n",
    "    Z1 = (W1 @ (X_test[i].reshape(15, 1))) + b1\n",
    "    A1 = expit(Z1)\n",
    "    Z2 = (W2 @ A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    prediction = np.argmax(A2)\n",
    "    label = np.argmax(y_test[i])\n",
    "\n",
    "    if(prediction == label):\n",
    "        Accuracy += 1\n",
    "\n",
    "Accuracy /= n_testDatapoints\n",
    "\n",
    "print(\"Accuracy : \",Accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
